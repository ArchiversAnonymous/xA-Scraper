"""empty message

Revision ID: 6937420ccb1b
Revises: None
Create Date: 2017-07-11 05:11:03.555910

"""

# revision identifiers, used by Alembic.
revision = '6937420ccb1b'
down_revision = None

from alembic import op
import sqlalchemy as sa
import citext
import datetime
from sqlalchemy.dialects import postgresql

def upgrade():
    ### commands auto generated by Alembic - please adjust! ###
    scrape_targets_tbl = op.create_table('scrape_targets',
    sa.Column('id', sa.BigInteger(), nullable=False),
    sa.Column('site_name', sa.Text(), nullable=False),
    sa.Column('artist_name', sa.Text(), nullable=False),
    sa.Column('uploadeh', sa.Boolean(), nullable=True),
    sa.Column('last_fetched', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('site_name', 'artist_name')
    )
    op.create_index(op.f('ix_scrape_targets_uploadeh'), 'scrape_targets', ['uploadeh'], unique=False)

    art_item_tbl = op.create_table('art_item',
    sa.Column('id', sa.BigInteger(), nullable=False),
    sa.Column('state', postgresql.ENUM('new', 'fetching', 'processing', 'complete', 'error', 'removed', 'disabled', 'specialty_deferred', 'specialty_ready', name='dlstate_enum'), nullable=False),
    sa.Column('errno', sa.Integer(), nullable=True),
    sa.Column('artist_id', sa.BigInteger(), nullable=True),
    sa.Column('release_meta', sa.Text(), nullable=False),
    sa.Column('fetchtime', sa.DateTime(), nullable=True),
    sa.Column('addtime', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['artist_id'], ['scrape_targets.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('artist_id', 'release_meta')
    )
    op.create_index(op.f('ix_art_item_id'), 'art_item', ['id'], unique=False)
    op.create_index(op.f('ix_art_item_state'), 'art_item', ['state'], unique=False)

    art_file_tbl = op.create_table('art_file',
    sa.Column('id', sa.BigInteger(), nullable=False),
    sa.Column('item_id', sa.BigInteger(), nullable=False),
    sa.Column('seqnum', sa.Integer(), nullable=False),
    sa.Column('title', sa.Text(), nullable=True),
    sa.Column('content', sa.Text(), nullable=True),
    sa.Column('filename', sa.Text(), nullable=True),
    sa.Column('fspath', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['item_id'], ['art_item.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('item_id', 'seqnum')
    )
    op.create_index(op.f('ix_art_file_id'), 'art_file', ['id'], unique=False)

    art_tag_tbl = op.create_table('art_tag',
    sa.Column('id', sa.BigInteger(), nullable=False),
    sa.Column('item_id', sa.BigInteger(), nullable=False),
    sa.Column('tag', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['item_id'], ['art_item.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('item_id', 'tag')
    )
    op.create_index(op.f('ix_art_tag_id'), 'art_tag', ['id'], unique=False)

    ##########################################################################################
    ##########################################################################################

    conn = op.get_bind()
    res = conn.execute('''
        SELECT
             sitename,
             artistname,
             uploadeh,
             last_fetched
        FROM siteartistnames;
        ''')
    data = res.fetchall()


    sources = [
        {
            'site_name': sitename,
            'artist_name':artistname,
            'uploadeh': bool(uploadeh),
            'last_fetched': datetime.datetime.utcfromtimestamp(last_fetched)
        }
        for
            sitename,
            artistname,
            uploadeh,
            last_fetched
        in
            data
        ]
    op.bulk_insert(scrape_targets_tbl, sources)

    res = conn.execute('''
        SELECT
             id,
             site_name,
             artist_name
        FROM scrape_targets;
        ''')
    raw_uid_map = res.fetchall()

    user_map = {(site, user) : rid for rid, site, user in raw_uid_map}

    ##########################################################################################
    ##########################################################################################

    # xa_downloader=# \d retrieved_pages
    #                               Table "public.retrieved_pages"
    #      Column      |  Type   |                          Modifiers
    # -----------------+---------+--------------------------------------------------------------
    #  id              | integer | not null default nextval('retrieved_pages_id_seq'::regclass)
    #  sitename        | text    | not null
    #  artistname      | text    | not null
    #  pageurl         | text    | not null
    #  retreivaltime   | real    | not null
    #  downloadpath    | text    |
    #  itempagecontent | text    |
    #  itempagetitle   | text    |
    #  seqnum          | integer |
    # Indexes:
    #     "retrieved_pages_pkey" PRIMARY KEY, btree (id)
    #     "retrieved_pages_sitename_artistname_pageurl_seqnum_key" UNIQUE CONSTRAINT, btree (sitename, artistname, pageurl, seqnum)
    #     "retrieved_pages_artistname_index" btree (artistname)
    #     "retrieved_pages_pageurl_index" btree (pageurl)
    #     "retrieved_pages_site_src_time_index" btree (sitename, retreivaltime)
    #     "retrieved_pages_time_index" btree (retreivaltime)

    print("Fetching all items")
    res = conn.execute('''
        SELECT
            sitename,
            artistname,
            pageurl,
            retreivaltime,
            downloadpath,
            itempagecontent,
            itempagetitle,
            seqnum
        FROM retrieved_pages;
        ''')
    data = res.fetchall()
    print("Found %s data rows" % len(data))

    grouped = {}
    for sitename, artistname, pageurl, retreivaltime, downloadpath, itempagecontent, itempagetitle, seqnum in data:
        # So... DA does deduplication on content (I think?), so reposts often get multiple references
        # to the same underlying URL.
        itemkey = (sitename, artistname, pageurl)
        grouped.setdefault(itemkey, [])
        grouped[itemkey].append((sitename, artistname, pageurl, retreivaltime, downloadpath, itempagecontent, itempagetitle, seqnum))

    print("Distinct release items: %s" % len(grouped))

    for key, value in grouped.items():
        if len(value) > 1:
            print()
            print(key)
            print(value)


    ##########################################################################################
    ##########################################################################################


    raise ValueError("Wat?")



def downgrade():
    ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_art_tag_id'), table_name='art_tag')
    op.drop_table('art_tag')
    op.drop_index(op.f('ix_art_file_id'), table_name='art_file')
    op.drop_table('art_file')
    op.drop_index(op.f('ix_art_item_state'), table_name='art_item')
    op.drop_index(op.f('ix_art_item_id'), table_name='art_item')
    op.drop_table('art_item')
    op.drop_index(op.f('ix_scrape_targets_uploadeh'), table_name='scrape_targets')
    op.drop_table('scrape_targets')
    ### end Alembic commands ###
